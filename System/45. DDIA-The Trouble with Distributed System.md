# The Trouble with Distributed System

There is no fundamental reason why software on a single computer should be **flaky**: when the hardware is working 
correctly, the same operation always produces the same result (it is **deterministic**). If there is a hardware problem 
(e.g., memory corruption or a loose connector), the consequence is usually a total system failure (e.g., kernel 
panic, “blue screen of death,” failure to start up). An individual computer with good software is usually either 
fully functional or entirely broken, but not something in between.

In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even 
though other parts of the system are working fine. This is known as a **partial failure**. The difficulty is that 
partial failures are **nondeterministic**: if you try to do anything involving multiple nodes and the network, it may 
sometimes work and sometimes unpredictably fail.

This nondeterminism and possibility of partial failures is what makes distributed systems hard to work with.

In distributed systems, we try to build **tolerance** of partial failures into software, so that the system as a whole 
may continue functioning even when some of its constituent parts are broken.

To tolerate faults, the first step is to **detect** them, but even that is hard. Most systems don’t have an accurate 
mechanism of detecting whether a node has failed, so most distributed algorithms rely on **timeouts** to determine 
whether a remote node is still available. However, timeouts can’t distinguish between network and node failures, and 
variable network delay sometimes causes a node to be falsely suspected of crashing.

Once a fault is detected, making a system **tolerate** it is not easy either: there is no global variable, no shared 
memory, no common knowledge or any other kind of shared state between the machines. Nodes can’t even agree on what 
time it is, let alone on anything more profound. The only way information can flow from one node to another is by 
sending it over the unreliable network. Major decisions cannot be safely made by a single node, so we require 
protocols that enlist **help** from other nodes and try to get a **quorum** to agree.

**Scalability** is not the only reason for wanting to use a distributed system. **Fault tolerance** and **low latency** (by 
placing data geographically close to users) are equally important goals, and those things cannot be achieved with a 
single node.

It is possible to give hard real-time response guarantees and bounded delays in networks, but doing so is very 
expensive and results in lower utilization of hardware resources. Most non-safety-critical systems choose cheap and 
unreliable over expensive and reliable.

## Unreliable Networks

The distributed systems we focus on are **shared-nothing** systems: i.e., a bunch of machines connected by 
a network. The network is the only way those machines can communicate—we assume that each machine has its own memory 
and disk, and one machine cannot access another machine’s memory or disk (except by making requests to a service 
over the network).

The internet and most internal networks in datacenters (often Ethernet) are asynchronous packet networks. In this 
kind of network, one node can send a message (a packet) to another node, but the network gives no guarantees as to 
when it will arrive, or whether it will arrive at all.

If you send a request and don’t get a response, it’s not possible to distinguish whether (a) the request was lost, 
(b) the remote node is down, or (c) the response was lost, as shown below.

Handling network faults doesn’t necessarily mean tolerating them: if your network is normally fairly reliable, a 
valid approach may be to simply show an error message to users while your network is experiencing problems. However, 
you do need to **know** how your software reacts to network problems and ensure that the system can recover from them. 
It may make sense to deliberately trigger network problems and **test** the system’s response.

Currently technology does not allow us to make any guarantees about delays or reliability of the network: 
we have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there’s no 
“correct” value for timeouts—they need to be determined **experimentally**.

## Unreliable Clocks

A **time-of-day clock** does what you intuitively expect of a clock: it returns the current date and time according to 
some calendar (also known as wall-clock time).

A **monotonic clock** is suitable for measuring a **duration** (time interval), such as a timeout or a service’s response 
time. The name comes from the fact that they are guaranteed to always move forward. However, the absolute value of 
the clock is meaningless: it might be the number of nanoseconds since the computer was started, or something 
similarly arbitrary. In particular, it makes no sense to compare monotonic clock values from two different computers,
because they don’t mean the same thing.

If you use software that requires synchronized clocks, it is essential that you also carefully **monitor** the clock 
offsets between all the machines. Any node whose clock drifts too far from the others should be declared dead and 
removed from the cluster.

The most common implementation of snapshot isolation requires a monotonically increasing **transaction ID**. If a write 
happened later than the snapshot (i.e., the write has a greater transaction ID than the snapshot), that write is 
invisible to the snapshot transaction. On a single-node database, a simple counter is sufficient for generating 
transaction IDs. However, when a database is distributed across many machines, potentially in multiple datacenters, 
a global, monotonically increasing transaction ID (across all partitions) is difficult to generate, because it 
requires coordination. The transaction ID must reflect causality: if transaction B reads a value that was written by 
transaction A, then B must have a higher transaction ID than A—otherwise, the snapshot would not be consistent.

Can we use the timestamps from synchronized time-of-day clocks as transaction IDs? If we could get the 
**synchronization** good enough, they would have the right properties: later transactions have a higher timestamp. The 
problem, of course, is the uncertainty about clock accuracy.

When writing multi-threaded code on a single machine, we have fairly good tools for making it thread-safe: mutexes, 
semaphores, atomic counters, lock-free data structures, blocking queues, and so on. Unfortunately, these tools don’t 
directly translate to distributed systems, because a distributed system has no shared memory—only messages sent over 
an unreliable network.

## Knowledge, Truth, and Lies

So far in this chapter we have explored the ways in which distributed systems are different from programs running on 
a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, 
and the systems may suffer from partial failures, unreliable clocks, and processing pauses.

A node in the network cannot know anything for sure—it can only make guesses based on the **messages** it receives (or 
doesn’t receive) via the network. A node can only find out what state another node is in (what data it has stored, 
whether it is correctly functioning, etc.) by exchanging messages with it. If a remote node doesn’t respond, there 
is no way of knowing what state it is in, because problems in the network cannot reliably be distinguished from 
problems at a node.

In a distributed system, we can state the **assumptions** we are making about the behavior (the system model) and design 
the actual system in such a way that it meets those assumptions. Algorithms can be proved to function correctly 
within a certain system model.

### The Truth Is Defined by the Majority. 

A node cannot necessarily trust its own judgment of a situation. A distributed 
system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system 
stuck and unable to recover. Instead, many distributed algorithms rely on a **quorum**, that is, **voting** among 
the nodes: : decisions require some minimum number of votes from several nodes in order to reduce the dependence on 
any one particular node.

That includes decisions about declaring nodes dead. If a quorum of nodes declares another node dead, then it must be 
considered dead, even if that node still very much feels alive.

### Byzantine Faults

In this book we assume that nodes are unreliable but honest: they may be slow or never respond (due to a fault), and 
their state may be outdated (due to a GC pause or network delays), but we assume that if a node does respond, it is 
telling the “truth”: to the best of its knowledge, it is playing by the rules of the protocol.

Distributed systems problems become much harder if there is a risk that nodes may “lie” (send arbitrary faulty or 
corrupted responses)—for example, if a node may claim to have received a particular message when in fact it didn’t. 
Such behavior is known as a **Byzantine fault**. A system is Byzantine fault-tolerant if it continues to operate 
correctly even if some of the nodes are malfunctioning and not obeying the protocol, or if malicious attackers are 
interfering with the network.

### System Model and Reality

With regard to **timing** assumptions, three system models are in common use:

- Synchronous model: The synchronous model assumes **bounded** network delay, bounded process pauses, and bounded clock 
  error.
- Partially synchronous model: Partial synchrony means that a system behaves like a synchronous system most of the 
  time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift.
- Asynchronous model: In this model, an algorithm is not allowed to make any timing assumptions—in fact, it does not 
  even have a clock (so it cannot use timeouts).

Moreover, besides timing issues, we have to consider **node** failures. The three most common system models for 
nodes are:

- Crash-stop faults: In the crash-stop model, an algorithm may assume that a node can fail in only one way, namely 
  by crashing. This means that the node may suddenly stop responding at any moment, and thereafter that node is gone 
  forever—it never comes back.
- Crash-recovery faults: We assume that nodes may crash at any moment, and perhaps start responding again after some 
  unknown time. In the crash-recovery model, nodes are assumed to have stable storage (i.e., nonvolatile disk 
  storage) that is preserved across crashes, while the in-memory state is assumed to be lost.
- Byzantine (arbitrary) faults: Nodes may do absolutely anything, including trying to trick and deceive other nodes, 
  as described in the last section.

For modeling real systems, the **partially synchronous** model with **crash-recovery** faults is generally the most 
useful model.

#### Correctness of an algorithm

To define what it means for an algorithm to be correct, we can describe its **properties**.

For example, the output of a sorting algorithm has the property that for any two distinct elements of the output 
list, the element further to the left is smaller than the element further to the right. That is simply a formal way 
of defining what it means for a list to be sorted.

Similarly, we can write down the properties we want of a distributed algorithm to define what it means to be correct.
For example, if we are generating fencing tokens for a lock (see “Fencing tokens”), we may require the algorithm to 
have the following properties:

- Uniqueness
No two requests for a fencing token return the same value.

- Monotonic sequence
If request x returned token tx, and request y returned token ty, and x completed before y began, then tx < ty.

- Availability
A node that requests a fencing token and does not crash eventually receives a response.

## References

Designing Data-Intensive Applications By Martin Kleppmann
